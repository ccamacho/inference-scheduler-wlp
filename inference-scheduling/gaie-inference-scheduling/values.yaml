inferenceExtension:
  replicas: 1
  logVerbosity: 5
  image:
    # Either image will work, you just need to bring the correct plugins per image. In this example we will bring the upstream default plugin
    ###################
    name: llm-d-inference-scheduler
    hub: ghcr.io/llm-d
    tag: v0.3.1-rc.1 # Working: v0.2.1
    # tag: v0.2.1
    ###################
    # name: epp
    # hub: registry.k8s.io/gateway-api-inference-extension
    # tag: v0.5.1
    ###################
    pullPolicy: Always
  extProcPort: 9002
  
  # OPTION 1: Use default plugins
  # pluginsConfigFile: "default-plugins-custom.yaml"
  # pluginsCustomConfig:
  #   default-plugins-custom.yaml: |
  #     apiVersion: inference.networking.x-k8s.io/v1alpha1
  #     kind: EndpointPickerConfig
  #     plugins:
  #     - type: low-queue-filter
  #       parameters:
  #         threshold: 128
  #     - type: lora-affinity-filter
  #       parameters:
  #         threshold: 0.999
  #     - type: least-queue-filter
  #     - type: least-kv-cache-filter
  #     - type: decision-tree-filter
  #       name: low-latency-filter
  #       parameters:
  #         current:
  #           pluginRef: low-queue-filter
  #         nextOnSuccess:
  #           decisionTree:
  #             current:
  #               pluginRef: lora-affinity-filter
  #             nextOnSuccessOrFailure:
  #               decisionTree:
  #                 current:
  #                   pluginRef: least-queue-filter
  #                 nextOnSuccessOrFailure:
  #                   decisionTree:
  #                     current:
  #                       pluginRef: least-kv-cache-filter
  #         nextOnFailure:
  #           decisionTree:
  #             current:
  #               pluginRef: least-queue-filter
  #             nextOnSuccessOrFailure:
  #               decisionTree:
  #                 current:
  #                   pluginRef: lora-affinity-filter
  #                 nextOnSuccessOrFailure:
  #                   decisionTree:
  #                     current:
  #                       pluginRef: least-kv-cache-filter
  #     - type: random-picker
  #       parameters:
  #         maxNumOfEndpoints: 1
  #     - type: single-profile-handler
  #     schedulingProfiles:
  #     - name: default
  #       plugins:
  #       - pluginRef: low-latency-filter
  #       - pluginRef: random-picker

  # OPTION 2: v2 custom scorers
  # pluginsConfigFile: "plugins-v2-custom.yaml"
  # pluginsCustomConfig:
  #   plugins-v2-custom.yaml: |
  #     apiVersion: inference.networking.x-k8s.io/v1alpha1
  #     kind: EndpointPickerConfig
  #     plugins:
  #     - type: queue-scorer
  #     - type: kv-cache-scorer
  #     - type: prefix-cache-scorer
  #       parameters:
  #         hashBlockSize: 64
  #         maxPrefixBlocksToMatch: 256
  #         lruCapacityPerServer: 31250
  #     - type: max-score-picker
  #       parameters:
  #         maxNumOfEndpoints: 1
  #     - type: single-profile-handler
  #     schedulingProfiles:
  #     - name: default
  #       plugins:
  #       - pluginRef: queue-scorer
  #         weight: 1
  #       - pluginRef: kv-cache-scorer
  #         weight: 1
  #       - pluginRef: prefix-cache-scorer
  #         weight: 1
  #       - pluginRef: max-score-picker

  pluginsConfigFile: "prefix-cache-tracking-config.yaml"
  pluginsCustomConfig:
    prefix-cache-tracking-config.yaml: |
      apiVersion: inference.networking.x-k8s.io/v1alpha1
      kind: EndpointPickerConfig
      plugins:
        - type: single-profile-handler
        - type: prefix-cache-scorer
          parameters:
            mode: cache_tracking
            indexerConfig:
              tokenProcessorConfig:
                blockSize: 64                         # must match vLLM block size if not default (16)
                hashSeed: "42"                        # must match PYTHONHASHSEED in vLLM pods
              kvBlockIndexConfig:
                enableMetrics: true                   # enable kv-block index metrics (prometheus)
                metricsLoggingInterval: 60000000000   # log kv-block metrics as well (1m in nanoseconds)
        - type: queue-scorer
        - type: weighted-random-picker
      schedulingProfiles:
        - name: default
          plugins:
            - pluginRef: prefix-cache-scorer
              weight: 100.0
            - pluginRef: queue-scorer
              weight: 1.0
            - pluginRef: weighted-random-picker

inferencePool:
  apiVersion: inference.networking.x-k8s.io/v1alpha2
  targetPortNumber: 8000
  modelServerType: vllm
  modelServers:
    matchLabels:
      llm-d.ai/inferenceServing: "true"
      # llm-d.ai/model: ms-simple-llm-d-modelservice
provider:
  name: none
