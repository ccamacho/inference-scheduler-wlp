inferenceExtension:
  replicas: 1
  logVerbosity: 5
  image:
    # Either image will work, you just need to bring the correct plugins per image. In this example we will bring the upstream default plugin
    ###################
    name: llm-d-inference-scheduler
    hub: ghcr.io/llm-d
    tag: v0.3.1-rc.1 # Working: v0.2.1
    # tag: v0.2.1
    ###################
    # name: epp
    # hub: registry.k8s.io/gateway-api-inference-extension
    # tag: v0.5.1
    ###################
    pullPolicy: Always
  extProcPort: 9002
  
  # OPTION 1: Use default plugins
  # pluginsConfigFile: "default-plugins-custom.yaml"
  # pluginsCustomConfig:
  #   default-plugins-custom.yaml: |
  #     apiVersion: inference.networking.x-k8s.io/v1alpha1
  #     kind: EndpointPickerConfig
  #     plugins:
  #     - type: low-queue-filter
  #       parameters:
  #         threshold: 128
  #     - type: lora-affinity-filter
  #       parameters:
  #         threshold: 0.999
  #     - type: least-queue-filter
  #     - type: least-kv-cache-filter
  #     - type: decision-tree-filter
  #       name: low-latency-filter
  #       parameters:
  #         current:
  #           pluginRef: low-queue-filter
  #         nextOnSuccess:
  #           decisionTree:
  #             current:
  #               pluginRef: lora-affinity-filter
  #             nextOnSuccessOrFailure:
  #               decisionTree:
  #                 current:
  #                   pluginRef: least-queue-filter
  #                 nextOnSuccessOrFailure:
  #                   decisionTree:
  #                     current:
  #                       pluginRef: least-kv-cache-filter
  #         nextOnFailure:
  #           decisionTree:
  #             current:
  #               pluginRef: least-queue-filter
  #             nextOnSuccessOrFailure:
  #               decisionTree:
  #                 current:
  #                   pluginRef: lora-affinity-filter
  #                 nextOnSuccessOrFailure:
  #                   decisionTree:
  #                     current:
  #                       pluginRef: least-kv-cache-filter
  #     - type: random-picker
  #       parameters:
  #         maxNumOfEndpoints: 1
  #     - type: single-profile-handler
  #     schedulingProfiles:
  #     - name: default
  #       plugins:
  #       - pluginRef: low-latency-filter
  #       - pluginRef: random-picker

  # OPTION 2: v2 custom scorers
  pluginsConfigFile: "plugins-v2-custom.yaml"
  pluginsCustomConfig:
    plugins-v2-custom.yaml: |
      apiVersion: inference.networking.x-k8s.io/v1alpha1
      kind: EndpointPickerConfig
      plugins:
      - type: queue-scorer
      - type: kv-cache-scorer
      - type: prefix-cache-scorer
        parameters:
          hashBlockSize: 64
          maxPrefixBlocksToMatch: 256
          lruCapacityPerServer: 31250
      - type: max-score-picker
        parameters:
          maxNumOfEndpoints: 1
      - type: single-profile-handler
      schedulingProfiles:
      - name: default
        plugins:
        - pluginRef: queue-scorer
          weight: 1
        - pluginRef: kv-cache-scorer
          weight: 1
        - pluginRef: prefix-cache-scorer
          weight: 1
        - pluginRef: max-score-picker

inferencePool:
  apiVersion: inference.networking.x-k8s.io/v1alpha2
  targetPortNumber: 8000
  modelServerType: vllm
  modelServers:
    matchLabels:
      llm-d.ai/inferenceServing: "true"
      # llm-d.ai/model: ms-simple-llm-d-modelservice
provider:
  name: none
