inferenceExtension:
  replicas: 1
  logVerbosity: 5
  image:
    # Either image will work, you just need to bring the correct plugins per image. In this example we will bring the upstream default plugin
    ###################
    name: llm-d-inference-scheduler
    hub: ghcr.io/llm-d
    # tag: v0.3.0-rc.2 # Working: v0.2.1
    tag: v0.2.1
    ###################
    # name: epp
    # hub: registry.k8s.io/gateway-api-inference-extension
    # tag: v0.5.1
    ###################
    pullPolicy: Always
  extProcPort: 9002
  
  # OPTION 1: Use default plugins
  # pluginsConfigFile: "default-plugins-custom.yaml"
  # pluginsCustomConfig:
  #   default-plugins-custom.yaml: |
  #     apiVersion: inference.networking.x-k8s.io/v1alpha1
  #     kind: EndpointPickerConfig
  #     plugins:
  #     - type: low-queue-filter
  #       parameters:
  #         threshold: 128
  #     - type: lora-affinity-filter
  #       parameters:
  #         threshold: 0.999
  #     - type: least-queue-filter
  #     - type: least-kv-cache-filter
  #     - type: decision-tree-filter
  #       name: low-latency-filter
  #       parameters:
  #         current:
  #           pluginRef: low-queue-filter
  #         nextOnSuccess:
  #           decisionTree:
  #             current:
  #               pluginRef: lora-affinity-filter
  #             nextOnSuccessOrFailure:
  #               decisionTree:
  #                 current:
  #                   pluginRef: least-queue-filter
  #                 nextOnSuccessOrFailure:
  #                   decisionTree:
  #                     current:
  #                       pluginRef: least-kv-cache-filter
  #         nextOnFailure:
  #           decisionTree:
  #             current:
  #               pluginRef: least-queue-filter
  #             nextOnSuccessOrFailure:
  #               decisionTree:
  #                 current:
  #                   pluginRef: lora-affinity-filter
  #                 nextOnSuccessOrFailure:
  #                   decisionTree:
  #                     current:
  #                       pluginRef: least-kv-cache-filter
  #     - type: random-picker
  #       parameters:
  #         maxNumOfEndpoints: 1
  #     - type: single-profile-handler
  #     schedulingProfiles:
  #     - name: default
  #       plugins:
  #       - pluginRef: low-latency-filter
  #       - pluginRef: random-picker

  # OPTION 2: v2 custom scorers
  pluginsConfigFile: "plugins-v2-custom.yaml"
  pluginsCustomConfig:
    plugins-v2-custom.yaml: |
      apiVersion: inference.networking.x-k8s.io/v1alpha1
      kind: EndpointPickerConfig
      plugins:
      - type: queue-scorer
      - type: kv-cache-scorer
      - type: prefix-cache-scorer
        parameters:
          hashBlockSize: 64
          maxPrefixBlocksToMatch: 256
          lruCapacityPerServer: 31250
      - type: max-score-picker
        parameters:
          maxNumOfEndpoints: 1
      - type: single-profile-handler
      schedulingProfiles:
      - name: default
        plugins:
        - pluginRef: queue-scorer
          weight: 1
        - pluginRef: kv-cache-scorer
          weight: 1
        - pluginRef: prefix-cache-scorer
          weight: 1
        - pluginRef: max-score-picker


# 2025-09-23T08:50:09.651480257Z 2025-09-23T08:50:09Z	LEVEL(-5)	metrics/pod_metrics.go:121	Failed to refreshed metrics:	{"pod": {"name":"ms-inference-scheduling-llm-d-modelservice-decode-695f9bd8cjbm7","namespace":"llm-d-inference-scheduling"}, "err": "metric family \"vllm:lora_requests_info\" not found"}
# 2025-09-23T08:50:09.651489638Z 2025-09-23T08:50:09Z	LEVEL(-5)	metrics/pod_metrics.go:133	Refreshed metrics	{"pod": {"name":"ms-inference-scheduling-llm-d-modelservice-decode-695f9bd8cjbm7","namespace":"llm-d-inference-scheduling"}, "updated": "{ActiveModels:map[] WaitingModels:map[] MaxActiveModels:0 RunningQueueSize:0 WaitingQueueSize:0 KVCacheUsagePercent:0 KvCacheMaxTokenCapacity:0 UpdateTime:2025-09-23 08:50:09.651469232 +0000 UTC m=+870.769831645}"}
# 2025-09-23T08:50:09.668471954Z 2025-09-23T08:50:09Z	LEVEL(-5)	metrics/pod_metrics.go:121	Failed to refreshed metrics:	{"pod": {"name":"ms-inference-scheduling-llm-d-modelservice-decode-695f9bd8h4st4","namespace":"llm-d-inference-scheduling"}, "err": "metric family \"vllm:lora_requests_info\" not found"}
# 2025-09-23T08:50:09.668471954Z 2025-09-23T08:50:09Z	LEVEL(-5)	metrics/pod_metrics.go:133	Refreshed metrics	{"pod": {"name":"ms-inference-scheduling-llm-d-modelservice-decode-695f9bd8h4st4","namespace":"llm-d-inference-scheduling"}, "updated": "{ActiveModels:map[] WaitingModels:map[] MaxActiveModels:0 RunningQueueSize:0 WaitingQueueSize:0 KVCacheUsagePercent:0 KvCacheMaxTokenCapacity:0 UpdateTime:2025-09-23 08:50:09.668460625 +0000 UTC m=+870.786823040}"}
# 2025-09-23T08:50:09.683373224Z 2025-09-23T08:50:09Z	LEVEL(-5)	metrics/pod_metrics.go:121	Failed to refreshed metrics:	{"pod": {"name":"ms-inference-scheduling-llm-d-modelservice-decode-695f9bd8qwxlt","namespace":"llm-d-inference-scheduling"}, "err": "metric family \"vllm:lora_requests_info\" not found"}
# 2025-09-23T08:50:09.683373224Z 2025-09-23T08:50:09Z	LEVEL(-5)	metrics/pod_metrics.go:133	Refreshed metrics	{"pod": {"name":"ms-inference-scheduling-llm-d-modelservice-decode-695f9bd8qwxlt","namespace":"llm-d-inference-scheduling"}, "updated": "{ActiveModels:map[] WaitingModels:map[] MaxActiveModels:0 RunningQueueSize:0 WaitingQueueSize:0 KVCacheUsagePercent:0 KvCacheMaxTokenCapacity:0 UpdateTime:2025-09-23 08:50:09.683361835 +0000 UTC m=+870.801724248}"}
# 2025-09-23T08:50:09.694489691Z 2025-09-23T08:50:09Z	LEVEL(-5)	metrics/pod_metrics.go:121	Failed to refreshed metrics:	{"pod": {"name":"ms-inference-scheduling-llm-d-modelservice-decode-695f9bd878bl8","namespace":"llm-d-inference-scheduling"}, "err": "metric family \"vllm:lora_requests_info\" not found"}
# 2025-09-23T08:50:09.694489691Z 2025-09-23T08:50:09Z	LEVEL(-5)	metrics/pod_metrics.go:133	Refreshed metrics	{"pod": {"name":"ms-inference-scheduling-llm-d-modelservice-decode-695f9bd878bl8","namespace":"llm-d-inference-scheduling"}, "updated": "{ActiveModels:map[] WaitingModels:map[] MaxActiveModels:0 RunningQueueSize:0 WaitingQueueSize:0 KVCacheUsagePercent:0 KvCacheMaxTokenCapacity:0 UpdateTime:2025-09-23 08:50:09.694478581 +0000 UTC m=+870.812840994}"}
# 2025-09-23T08:50:09.700516030Z 2025-09-23T08:50:09Z	LEVEL(-5)	metrics/pod_metrics.go:121	Failed to refreshed metrics:	{"pod": {"name":"ms-inference-scheduling-llm-d-modelservice-decode-695f9bd8cjbm7","namespace":"llm-d-inference-scheduling"}, "err": "metric family \"vllm:lora_requests_info\" not found"}
# 2025-09-23T08:50:09.700516030Z 2025-09-23T08:50:09Z	LEVEL(-5)	metrics/pod_metrics.go:133	Refreshed metrics	{"pod": {"name":"ms-inference-scheduling-llm-d-modelservice-decode-695f9bd8cjbm7","namespace":"llm-d-inference-scheduling"}, "updated": "{ActiveModels:map[] WaitingModels:map[] MaxActiveModels:0 RunningQueueSize:0 WaitingQueueSize:0 KVCacheUsagePercent:0 KvCacheMaxTokenCapacity:0 UpdateTime:2025-09-23 08:50:09.700505 +0000 UTC m=+870.818867414}"}


inferencePool:
  targetPortNumber: 8000
  modelServerType: vllm
  modelServers:
    matchLabels:
      llm-d.ai/inferenceServing: "true"
      # llm-d.ai/model: ms-simple-llm-d-modelservice
provider:
  name: none
